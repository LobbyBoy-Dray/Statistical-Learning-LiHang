{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SVM, support vector machine:\n",
    "> \n",
    "> * 线性可分SVM = Hard-margin\n",
    "> * 线性SVM = Soft-margin\n",
    "> * 非线性SVM = Soft-margin + Kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 线性可分SVM与Hard-margin最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 线性可分支持向量机: Linear seperable SVM，适用于线性可分的训练集，否则无解。\n",
    "> \n",
    "> 硬间隔最大化: Hard-margin SVM，要求SVM对所有实例分类正确，不能有一个错分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 线性可分SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑二元分类问题(binary classification)，$y \\in \\mathcal{Y} = \\{+1,-1\\}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入空间(input space)和特征空间(feature space)不同。前者是欧式空间或离散集合，后者是欧式空间或希尔伯特空间。input space与feature space中的元素是__一一对应__的：一个input，对应一个feature space中的feature vector。SVM是在feature space中进行学习(可以理解为feature transformation)。所以，我们现在所说的训练数据集$T$，指代的是feature space。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设：$T$线性可分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM通过学习，将在feature space中找到一个__分离超平面(seperating hyperplane)__。这个分离超平面满足两个性质：\n",
    "\n",
    "* 将所有的实例正确分类；\n",
    "* 与训练数据有最大的间隔，即largest margin(暂时先不管这个词的含义)。\n",
    "\n",
    "是唯一的。\n",
    "\n",
    "该largest-margin seperting hyperplane记做$w^*·x+b^*=0$；相应的，分类决策函数为$f(x)=sign(w^*·x+b^*)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__那什么是margin? 什么是largest margin?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](.\\pic\\ch7-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(在线性可分的假设下)满足“将所有实例正确分类的”分离超平面有无数个(上图中便有三个)。它们的$E_{in}$都是0，看起来没有差别。但是直觉会告诉我们，右边的那条线好一些，因为该hyperplane与训练数据“看起来”比较远，因此能够容忍更大的杂讯，也就更加robust，不易overfit。\n",
    "\n",
    "我们可以这样理解一个hyperplane的margin：从该hyperplane开始，逐渐往外“长”，“长”多少会碰到第一个实例——这里，“长”的多少，就是margin——margin = distance to closest $x_n$。从上图可以看出，最右边的hyperplane，有着最大的margin，因此它的distance to closest $x_n$最大(灰色地方的宽度)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更加正式一些，我们将定义__函数间隔(functional margin)__与__几何间隔(geometric margin)__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 函数间隔 & 几何间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个分离超平面$w·x+b=0$，我们可以用$(w,b)$来表示。一个点与分离超平面越近，我们对该平面给予它的分类越不那么确信，同时$|w·x+b|$会越小；一个点与分离超平面越大，我们会愈加确信该平面给予它的分类，同时$|w·x+b|$越大——__一个点与分离超平面的远近，可以表示分类预测的确信程度__。同时，由于$|w·x+b|$能够相对应的表示点与分离超平面的远近(越大则越远，越小则越近)，因此$|w·x+b|$也能够表示对该点分类的确信程度。另外，如果该点被分类正确，则$y$应与$w·x+b$同号，否则异号。综上，$y(w·x+b)$既能表示分类的正确性，又能表示确信度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们这样定义函数间隔：\n",
    "\n",
    "1. 超平面$(w,b)$与实例$(x_i,y_i)$的函数间隔为：\n",
    "\n",
    "$$\\hat{\\gamma}_i = y_i(w·x_i+b)$$\n",
    "\n",
    "2. 超平面$(w,b)$与数据集$T$的函数间隔为：\n",
    "\n",
    "$$\\hat{\\gamma} = \\underset{i=1,2,\\cdots,N}{min}\\hat{\\gamma}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但函数间隔有一个问题，即如果成比例地改变$w$与$b$(如$(w,b)→(\\lambda w,\\lambda b)$)，超平面并没有改变，因为$w·x+b=0$与$\\lambda w·x+\\lambda b=0$其实是一个超平面，但函数间隔却会改变，变为原来的$\\lambda$倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，最大化间隔，不能直接最大化函数间隔，否则没有意义(无解)——$w$和$b$都取无穷大。需要给予一定的约束，如$||w||=1$，相当于__标准化__。这时，我们可以定义__几何间隔__：\n",
    "\n",
    "1. 超平面$(w,b)$与实例$(x_i,y_i)$的几何间隔为：\n",
    "\n",
    "$$\\gamma_i = \\frac{y_i(w·x_i+b)}{||w||}$$\n",
    "\n",
    "2. 超平面$(w,b)$与数据集$T$的函数间隔为：\n",
    "\n",
    "$$\\gamma = \\underset{i=1,2,\\cdots,N}{min}\\gamma_i$$\n",
    "\n",
    "参考点到平面的距离公式，超平面与实例的几何间隔表示的是实例点到超平面的__带符号的距离__：绝对值表示距离大小，符号表示是否分类正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 间隔最大化: largest margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性可分SVM最大化的，是__几何间隔__，即：求解能够正确划分训练数据集(hard-margin)并且几何间隔最大(largest-margin)的分离超平面。这样唯一的超平面的最大优点，是它对于最难分的实例点(离超平面最近的点)，也有足够大的确信度将其分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将上述问题数学化表示：\n",
    "\n",
    "$$\\underset{w,b}{max}\\ \\gamma$$\n",
    "\n",
    "$$s.t.\\ \\frac{y_i(w·x_i+b)}{||w||} \\ge \\gamma,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为函数间隔与几何间隔存在如下关系：\n",
    "\n",
    "$$\\gamma=\\frac{\\hat{\\gamma}}{||w||}$$\n",
    "\n",
    "我们将上述代入原最优化问题，得到等价问题：\n",
    "\n",
    "$$\\underset{w,b}{max}\\ \\frac{\\hat{\\gamma}}{||w||}$$\n",
    "\n",
    "$$s.t.\\ y_i(w·x_i+b) \\ge \\hat{\\gamma},\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于函数间隔的大小，并不影响原最优化问题，因此我们可以随意设置函数间隔大小，得到的最优化问题仍然和原问题等价。那么不妨设$\\hat{\\gamma}=1$：\n",
    "\n",
    "$$\\underset{w,b}{max}\\ \\frac{1}{||w||}$$\n",
    "\n",
    "$$s.t.\\ y_i(w·x_i+b) \\ge 1,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稍微修改一下目标函数，将最大化转化为最小化：\n",
    "\n",
    "$$\\underset{w,b}{min}\\ \\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.\\ y_i(w·x_i+b) \\ge 1,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述最优化问题，即为线性可分SVM的最优化问题——也是一个__凸二次规划问题(convex quadratic programming)__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "              & \\underset{x \\in \\mathbf{R}^n}{min}\\ \\ f(x)\\\\\n",
    "    s.t.\\ \\ \\ & c_i(x) \\le 0,\\quad i=1,2,\\cdots,k\\\\\n",
    "              & h_j(x) = 0,\\quad j=1,2,\\cdots,l\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__凸优化问题__：\n",
    "\n",
    "* $f(x)$与$c_i(x)$都是连续可微的凸函数；\n",
    "* $h_j(x)$是仿射函数——$f(x)=a·x+b$，那么$f(x)$就是仿射函数。\n",
    "\n",
    "__凸二次规划问题__：\n",
    "\n",
    "* 凸优化问题；\n",
    "* $f(x)$是二次函数；\n",
    "* $c_i(x)$是仿射函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有专门解决凸二次规划问题的程序，十分容易解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "落在边界上的点，被称为support vector(candidates)。它们满足：\n",
    "\n",
    "$$|y_i(w·x_i+b)|=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两条边界分别为：\n",
    "\n",
    "$$y(w·x+b)=1$$\n",
    "$$y(w·x+b)=-1$$\n",
    "\n",
    "两条边界之间不会有任何实例点，他们的距离为:\n",
    "\n",
    "$$\\frac{2}{||w||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据集中的支持向量决定着唯一的那个最大间隔的超平面，其他实例点并不重要：\n",
    "\n",
    "* 去掉非支持向量，解不变；\n",
    "* 去掉支持向量，解可能改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 对偶问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们往往不解原问题，而解对偶问题？\n",
    "\n",
    "* dual往往更容易求解；\n",
    "* 能够使用kernel trick。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: primal problem\n",
    "\n",
    "$$\\underset{w,b}{min}\\ \\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.\\ y_i(w·x_i+b) \\ge 1,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入拉格朗日乘子，构造拉格朗日函数：\n",
    "\n",
    "$$L(w,b,\\alpha) = \\frac{1}{2}||w||^2+\\sum_{i=1}^N\\alpha_i\\Big(1-y_i(w·x_i+b)\\Big),\\quad \\alpha_i \\ge 0$$\n",
    "\n",
    "primal problem等价于：\n",
    "\n",
    "$$\\underset{w,b}{min}\\ \\underset{\\alpha;\\alpha_i\\ge0}{max}\\ L(w,b,\\alpha)$$\n",
    "\n",
    "其dual problem为：\n",
    "\n",
    "$$\\underset{\\alpha;\\alpha_i\\ge0}{max}\\ \\underset{w,b}{min}\\ L(w,b,\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以继续化简dual problem：对于内层的最小化问题，我们分别对$\\nabla_w L$与$\\nabla_b L$置零，得到：\n",
    "\n",
    "$$w=\\sum_{i=1}^N \\alpha_iy_ix_i$$\n",
    "\n",
    "$$\\sum_{i=1}^N \\alpha_iy_i=0$$\n",
    "\n",
    "将上面两个等式代入$L$中，可以将dual problem转化为：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{\\alpha}{max}\\ -\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j(x_i·x_j)+\\sum_i\\alpha_i\\\\\n",
    "    s.t.\\ \\ \\ & \\alpha_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\sum_i\\alpha_iy_i=0\n",
    "\\end{align*}\n",
    "\n",
    "上述问题被称为__standard hard-margin SVM dual__(已被证明，SVM中的primal与dual满足强对偶关系，是等价的，因此我们解dual即可)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解得$\\alpha^*$后，我们可以运用KKT条件，解$w^*$与$b^*$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由$\\nabla_w L=0$得：\n",
    "\n",
    "$$w^* = \\sum_i\\alpha_i^*y_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到KKT条件中的：\n",
    "\n",
    "$$\\alpha_i^*\\Big(1-y_i(w^*·x_i+b^*)\\Big)=0,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明，至少存在一个$\\alpha_j>0$：(反证法)若$\\alpha^*=0$，则$w^*=0$，不成立。那么对应的，有：\n",
    "\n",
    "$$1-y_j(w^*·x_j+b^*)=0$$\n",
    "\n",
    "得：\n",
    "\n",
    "$$b^* = y_j - w^*x_j = y_j - \\sum_i\\alpha_i^*y_ix_i·x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们再次定义支持向量：我们将$\\alpha^*_i>0$对应的样本点$(x_i,y_i)$称为support vector。根据互补条件，我们知道，support vector一定在边界上(但在边界上的样本点不一定是support vector)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 线性SVM与Soft-margin最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 但由于noise的存在，我们的训练数据集往往并不是线性可分的；\n",
    ">\n",
    "> 并且，即使线性可分，如果我们坚持将资料分开而不犯一个错，往往会因noise而造成overfit。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 初步改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: primal problem\n",
    "\n",
    "$$\\underset{w,b}{min}\\ \\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.\\ y_i(w·x_i+b) \\ge 1,\\quad i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试改造一下上面的最优化问题，使得我们放弃对一些点分类正确的要求。可以：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{w,b}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i I\\Big(y_i \\ne sign(w·x_i+b)\\Big)\\\\\n",
    "    s.t.\\ \\ \\ & \\ y_i(w·x_i+b) \\ge 1,\\quad for\\ correct\\ i\\\\\n",
    "              & \\ y_i(w·x_i+b) \\ge -\\infty,\\quad for\\ incorrect\\ i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述最优化问题中，$C$后面的求和项表示__犯错点的总数__，而$C$类似“调节旋钮”: trade off of __large margin__ & __noise tolerance__\n",
    "\n",
    "* large $C$：对犯错点数量的惩罚大，因此模型倾向于少犯错，即使margin需要小一些——易过拟合；\n",
    "* small $C$：对犯错点数量的惩罚小，因此模型倾向于多犯错，使margin大一些——易欠拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以继续改写成：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{w,b}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i I\\Big(y_i \\ne sign(w·x_i+b)\\Big)\\\\\n",
    "    s.t.\\ \\ \\ & \\ y_i(w·x_i+b) \\ge 1-\\infty·I\\Big(y_i \\ne sign(w·x_i+b)\\Big),\\quad i=1,2,\\cdots,N\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是上述最优化问题有两个缺点：\n",
    "1. 非QP问题；\n",
    "2. 无法区分large error与small error。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 引入松弛变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "              & \\underset{w,b,\\xi}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i \\xi_i\\\\\n",
    "    s.t.\\ \\ \\ & \\ y_i(w·x_i+b) \\ge 1-\\xi_i,\\quad i=1,2,\\cdots,N\\\\\n",
    "        \\ \\ \\ & \\xi_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此前我们要求对于每一个样本$(x_i,y_i)$，其必须位于边界上或边界外且不能分类错误，即必须满足$y_i(w·x_i+b) \\ge 1$。但现在，我们给每一个样本配置一个松弛变量$\\xi_i \\ge 0$，不强求$y_i(w·x_i+b) \\ge 1$，只需要$y_i(w·x_i+b) \\ge 1-\\xi_i$即可。这说明，该样本点可以位于边界内部，甚至可以分类错误，这取决于$\\xi_i$的大小：\n",
    "\n",
    "* $\\xi_i$大，说明犯的错越大；\n",
    "* $\\xi_i$大，说明犯的错越小；\n",
    "\n",
    "而把$\\xi_i$的和作为最小化项目的一部分，说明我们还是尽量要分类正确+位于边界上或外，也就是我们还是要尽量使$\\xi_i$小的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到，引入松弛变量后的最优化问题，依然是凸二次规划问题，$w$的解唯一，但$b$的解可能不唯一，而是存在于一个区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 线性SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解引入松弛变量$\\xi$后的最优化问题：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{w,b,\\xi}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i \\xi_i\\\\\n",
    "    s.t.\\ \\ \\ & \\ y_i(w·x_i+b) \\ge 1-\\xi_i,\\quad i=1,2,\\cdots,N\\\\\n",
    "        \\ \\ \\ & \\xi_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "\\end{align*}\n",
    "\n",
    "得到的$(w^*,b^*)$，称为线性SVM，也叫做Soft-Margin SVM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 对偶问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拉格朗日函数：\n",
    "\n",
    "$$L(w,b,\\xi,\\alpha,\\beta)=\\frac{1}{2}||w||^2+C\\sum_i\\xi_i-\\sum_i\\alpha_i\\Big(y_i(w·x_i+b)-1+\\xi_i\\Big)-\\sum_i\\beta_i\\xi_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__primal__:\n",
    "\n",
    "$$\\underset{w,b,\\xi}{min}\\ \\underset{\\alpha,\\beta;\\alpha_i\\ge0,\\beta_i\\ge0}{max}\\ L(w,b,\\xi,\\alpha,\\beta)$$\n",
    "\n",
    "__dual__:\n",
    "\n",
    "$$\\underset{\\alpha,\\beta;\\alpha_i\\ge0,\\beta_i\\ge0}{max}\\ \\underset{w,b,\\xi}{min}\\ L(w,b,\\xi,\\alpha,\\beta)$$\n",
    "\n",
    "同样，强对偶成立，等价，存在$(w^*,b^*,\\xi^*,\\alpha^*,\\beta^*)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于内层最小化问题，将下面三式分别置零：\n",
    "\n",
    "* $\\nabla_wL$: $w=\\sum_i \\alpha_iy_ix_i$\n",
    "* $\\nabla_bL$: $\\sum_i \\alpha_iy_i=0$\n",
    "* $\\nabla_{\\xi_i}L$: $\\alpha_i+\\beta_i=C$\n",
    "\n",
    "带回dual，化简得：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{\\alpha,\\beta}{max}\\ -\\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j(x_i·x_j)+\\sum_i\\alpha_i\\\\\n",
    "    s.t.\\ \\ \\ & \\alpha_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\beta_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\alpha_i+\\beta_i = C,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\sum_i\\alpha_iy_i=0\\\\\n",
    "              & \\Big(w=\\sum_i\\alpha_iy_ix_i\\Big)\n",
    "\\end{align*}\n",
    "\n",
    "将第二个约束和第三个约束结合，消去$\\beta$，得到最终的dual：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{\\alpha}{min}\\ \\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum_i\\alpha_i\\\\\n",
    "    s.t.\\ \\ \\ & 0 \\le \\alpha_i \\le C,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\sum_i\\alpha_iy_i=0\\\\\n",
    "              & \\Big(w=\\sum_i\\alpha_iy_ix_i\\Big)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KKT条件：\n",
    "\n",
    "* $\\alpha^*_i\\Big(y_i(w^*·x_i+b)-1+\\xi_i^*\\Big) = 0$\n",
    "* $\\alpha^*_i \\ge 0$\n",
    "* $y_i(w^*·x_i+b)-1+\\xi_i^* \\ge 0$\n",
    "\n",
    "* $\\beta_i^*\\xi_i^* = 0$\n",
    "* $\\beta_i^*\\ge 0$\n",
    "* $\\xi_i^*\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若存在$0 < \\alpha^*_j < C$，则根据$\\alpha_j+\\beta_j=C$，有$\\beta_j > 0$；因此$\\xi_j=0$且$y_i(w^*·x_i+b)-1+\\xi_i^*=0$；得$b^* = y_j - \\sum_iy_i\\alpha^*_j(x_i·x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还是从$\\alpha$的视角来看support vector:\n",
    "\n",
    "* Non-SV，即$\\alpha_i^*=0$，的样本点：\n",
    "    * 根据$\\alpha_i+\\beta_i=C$，$\\beta^*_i$大于零，所以$\\xi_i^*=0$；\n",
    "    * 由于$\\alpha_i^*=0$，不能确认$y_i(w^*·x_i+b)-1=0$；\n",
    "    * 没有违反margin，在margin外或上；\n",
    "* Free-SV，即$0<\\alpha_i^*<C$的样本点：\n",
    "    * 根据$\\alpha_i+\\beta_i=C$，$\\beta^*_i$大于零，所以$\\xi_i^*=0$；\n",
    "    * 由于$\\alpha_i^*>0$，可以确认$y_i(w^*·x_i+b)-1=0$；\n",
    "    * 没有违反margin，在margin上；\n",
    "* Bounded-SV，即$\\alpha_i^*=C$的样本点：\n",
    "    * 根据$\\alpha_i+\\beta_i=C$，$\\beta^*_i$等于零；\n",
    "    * 所以$\\xi_i^* \\ge 0$；\n",
    "    * 有违反margin，或在margin上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 合页损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft-Margin SVM中的$\\xi$被称为margin violation，它记录的是某个点犯错(违背margin)的情况：\n",
    "\n",
    "* 若没有犯错，则$\\xi_i=0$，此时$y_i(w·x_i+b)\\ge1$，因此$1-y_i(w·x_i+b)\\le0$；\n",
    "* 若犯错，甚至到了对面，则$\\xi_i=1-y_i(w·x_i+b)>0$，此时$y_i(w·x_i+b)<1$，因此$1-y_i(w·x_i+b)>0$；\n",
    "\n",
    "综上：\n",
    "\n",
    "$$\\xi_i = max\\Big(1-y_i(w·x_i+b),0\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft-Margin SVM的原始问题:\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{w,b,\\xi}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i \\xi_i\\\\\n",
    "    s.t.\\ \\ \\ & \\ y_i(w·x_i+b) \\ge 1-\\xi_i,\\quad i=1,2,\\cdots,N\\\\\n",
    "        \\ \\ \\ & \\xi_i \\ge 0,\\quad i=1,2,\\cdots,N\\\\\n",
    "\\end{align*}\n",
    "\n",
    "等价于如下的无约束最优化问题:\n",
    "\n",
    "$$\\underset{b,w}{min}\\ \\frac{1}{2}||w||^2+C\\sum_i max\\Big(1-y_i(w·x_i+b),0\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述最优化问题的目标函数，类似于，__L2正则化__：\n",
    "\n",
    "$$min\\ \\frac{\\lambda}{N}||w||^2+\\frac{1}{N}\\sum err$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们可以将Soft-Margin SVM看做以$max\\Big(1-y_i(w·x_i+b),0\\Big)$为single lost function的结构风险(L2)最小化。这个$max$函数，又被称为__合页损失函数__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们将Soft-margin SVM与以特定$\\hat{err}$的L2正则化联系了起来：\n",
    "\n",
    "* large margin = fewer hyperplanes = L2 reg of short $w$\n",
    "* larger $C$ = smaller $\\lambda$ = less reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. 非线性SVM与核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于__训练集(输入空间)非线性可分__的情况，我们往往采取的是__非线性变换(Non-linear transformation)__，将输入空间的训练集映射到特征空间(往往是更高维度的)，然后在特征空间训练一个线性模型，如SVM，进行分类。\n",
    "\n",
    "这也是我们在一开始就强调的，我们的SVM是在特征空间中训练的，而非输入空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了区分，我们用$x$表示输入空间中的样例，用$z$表示特征空间中的样例，$z = \\phi(x)$。此外，我们记$z$的维度为$\\tilde{d}$($x$的维度为$n$)，且一般来说$n<<\\tilde{d}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个很大的$\\tilde{d}$非常烦人，我们把问题从primal转化为dual，就是因为$\\tilde{d}$太大了，对应的QP问题不好解——$\\tilde{d}+1$个变量，$N$个约束；转成dual后，变成了$N$个变量，$N+1$个约束的QP问题——看似通过将primal转成dual，我们将$\\tilde{d}$消掉了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，在解dual的时候，我们要向解QP的程式传入矩阵$Q_D$，其元素$q_{n,m}=y_ny_mz_n^Tz_m$——要算$z_n^Tz_m$，还是$\\tilde{d}$维的运算……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为$z_n^Tz_m = \\phi(x_n)^T\\phi(x_m)$，所以计算$z_n^Tz_m$实际上是两步：\n",
    "\n",
    "1. 非线性转换——麻烦，要转化到很高很高的维度；\n",
    "2. 算内积——麻烦，维度很大很大的内积；\n",
    "\n",
    "能不能将两步结合在一起，直接在低维度进行运算？Kernel Trick的思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以2次转换为例，进行探索(因为有平方，所以这里我们把维度的角标变成下标)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Phi_2(x) = \\Big( 1,x_1, x_2, \\cdots, x_d, x_1^2, x_1x_2, \\cdots, x_1x_d, x_2x_1,x_2^2,\\cdots,x_2x_d,\\cdots,x_d^2\\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有两个点$x$与$x'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\Phi_2(x)^T\\Phi_2(x')\t& = 1+\\sum_{i=1}^d x_ix_i'+\\sum_{i=1}^d\\sum_{j=1}^dx_ix_jx_i'x_j'\\\\\n",
    "                            & = 1+\\sum_{i=1}^d x_ix_i'+\\sum_{i=1}^dx_ix_i'\\sum_{j=1}^dx_jx_j'\\\\\n",
    "                            & = 1+x^Tx'+(x^Tx')^2\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，我们好像只需要进行低维度的内积运算，就可以得到原本必须经过两步高维度操作才能得到的结果——Kernel = Transform + Inner product——两步合成一步！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernel Function:__\n",
    "\n",
    "$$K_{\\Phi}(x,x') = \\Phi(x)^T\\Phi(x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个Kernel Function对应着一个转换(但不是唯一的)，但我们可以不用关心这个转换，直接用此核函数计算原来要计算的$z_n^Tz_m$：例如，针对2次转换，原来，我们是要先把每个样本进行2次转换(费时1)，然后再成对成对算内积(费时2)；现在，我们直接用核函数，先成对算低维度内积，然后进行$1+a+a^2$的“数字运算”即可——省时：\n",
    "\n",
    "* 不用算$z$\n",
    "* 不用存$w$\n",
    "\n",
    "$$z_n^Tz_m = \\phi(x_n)^T\\phi(x_m) = K(x_n,x_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用在SVM中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{n,m} = y_ny_mz_n^Tz_m = y_ny_mK(x_n,x_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    b & = y_s - w^Tz_s\\\\\n",
    "      & = y_s - \\Big(\\sum_{i=1}^N\\alpha_iy_iz_i\\Big)^Tz_s\\\\\n",
    "      & = y_s - \\sum_{i=1}^N\\alpha_iy_iK(x_i,x_s)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    g_{SVM}(x) & = sign\\Big(w^T\\phi(x)+b\\Big)\\\\\n",
    "               & = sign\\Big(\\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)^T\\phi(x)+b\\Big)\\\\\n",
    "               & = sign\\Big(\\sum_{i=1}^N\\alpha_iy_iK(x_i,x)+b\\Big)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gernel Polynomial Kernel ($\\gamma > 0,\\ \\zeta\\ge0$):\n",
    "\n",
    "$$K_Q(x,x')=(\\zeta+\\gamma x^Tx')^Q$$\n",
    "\n",
    "其背后对应了一个$\\Phi_Q$，但不重要了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Kernel ($\\gamma = 0,\\ \\zeta = 0,\\ Q=1$):\n",
    "\n",
    "$$K_1(x,x')=x^Tx'$$\n",
    "\n",
    "相当于没有$\\Phi$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又称为RBF核(Radial Basis Function Kernel):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K(x,x')=exp(-\\gamma||x-x'||^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    g_{SVM}(x) & = sign\\Big(\\sum_{SV}\\alpha_iy_iK(x_i,x)+b\\Big)\\\\\n",
    "               & = sign\\Big(\\sum_{SV}\\alpha_iy_iexp(-\\gamma||x_i-x||^2)+b\\Big)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，高斯核的SVM，实质上是一个个以support vector为中心的高斯分布的线性组合；而$\\gamma$越大，对应的Gaussian分布越尖；因此，$\\gamma$越大，模型越容易overfit。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Kernel的比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Kernel: $K(x,x')=x^Tx'$\n",
    "    * 【优】safe，不易过拟合；\n",
    "    * 【优】fast，计算量小；\n",
    "    * 【优】explainable，能拿到$w$；\n",
    "    * 【缺】欠拟合；\n",
    "2. Polynomial Kernel\n",
    "    * 【优】拟合能力强一些；\n",
    "    * 【缺】Q很大时，数值计算困难；\n",
    "    * 【缺】有三个参数要调……\n",
    "3. Gaussian Kernel\n",
    "    * 【优】拟合能力更强了；\n",
    "    * 【优】只有一个参数；\n",
    "    * 【缺】拿不到$w$；\n",
    "    * 【缺】比linear慢，计算量稍大；\n",
    "    * 【缺】过拟合风险；\n",
    "4. Self-designed Kernel\n",
    "    * $K$是核函数的充要条件：①对称函数；②对任意数据集$D^N$，矩阵$K^{N×N}:K_{m,n}=K(x_m,x_n)$是半正定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. 序列最小最优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列最小最优化算法，Sequential Minimal Optimization，是针对求解SVM的dual开发出的算法。当然，SVM的dual是一个凸二次规划问题，可以使用传统的QP算法求解。但是，当__训练样本的容量非常大__时，传统的QP算法会非常缓慢。而SMO算法，是专门为SVM设计更加高效的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO要解决的是Soft-margin svm dual:\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{\\alpha}{min}\\ \\frac{1}{2}\\sum_i\\sum_j\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_i\\alpha_i\\\\\n",
    "    s.t.\\ \\ \\ & 0 \\le \\alpha_i \\le C,\\quad i=1,2,\\cdots,N\\\\\n",
    "              & \\sum_i\\alpha_iy_i=0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO的基本思路：\n",
    "\n",
    "* 如果所有的$\\alpha_i$都满足KKT条件，则$\\alpha$就是该最优化问题的解；\n",
    "* 否则，选择两个变量，如$\\alpha_1$与$\\alpha_2$，固定其他变量，针对这两个变量构建一个__子二次规划问题__——可以通过解析法求解；\n",
    "    * 两个变量的选择：第一个变量，是违反KKT条件最严重的那一个；第二个变量，是由约束条件自动确定的；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 两个变量的二次规划问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设选择的两个变量是$\\alpha_1,\\alpha_2$，其他变量$\\alpha_i(i=1,2,\\cdots,N)$是固定的。于是我们可以得到如下的子二次规划问题(将原目标函数中完全不含$\\alpha_1$和$\\alpha_2$的项目删去)：\n",
    "\n",
    "\\begin{align*}\n",
    "              & \\underset{\\alpha_1,\\alpha_2}{min}\\ W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_i\\alpha_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^Ny_i\\alpha_iK_{i2}\\\\   \n",
    "    s.t.\\ \\ \\ & 0 \\le \\alpha_i \\le C,\\quad i=1,2\\\\\n",
    "              & \\alpha_1y_1+\\alpha_2y_2 = -\\sum_{i=3}^N\\alpha_iy_i=\\zeta\\\\\n",
    "\\end{align*}\n",
    "\n",
    "其中，$K_{ij}=K(x_i,x_j)$，$\\zeta$是常数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为仅有两个变量$\\alpha_1$与$\\alpha_2$，所以上述子问题的约束可以用二维空间的图形表示：\n",
    "\n",
    "![](./pic/ch7-2.png)\n",
    "\n",
    "不等式约束将$\\alpha_1$与$\\alpha_2$框在边长为$C$的正方形内；等式约束使$\\alpha_1$与$\\alpha_2$位于平行与盒子对角线的直线上。两者结合，则把$\\alpha_1$与$\\alpha_2$约束在__一条平行于对角线的线段上__。\n",
    "\n",
    "因为确定了$\\alpha_1$与$\\alpha_2$中的一个，另一个可以由等式约束求出，因此该问题实质上也是单变量最优化问题——不妨考虑$\\alpha_2$为主变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设子问题的__初始可行解(上一轮的解，所以叫old)__为$(\\alpha_1^{old},\\alpha_2^{old})$，最优解为$(\\alpha_1^{new},\\alpha_2^{new})$，沿着约束方向但不受盒子约束时(在直线上，而不一定必须在线段上)$\\alpha_2$的最优解为$\\alpha_2^{new,unc}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，最优解$\\alpha_2^{new}$肯定要满足不等式约束(被框在盒子里)，因此$\\alpha_2^{new}$需要满足($L$和$H$是$\\alpha^{new}_2$所在的对角线端点的界)：\n",
    "\n",
    "$$L \\le \\alpha_2^{new} \\le H$$\n",
    "\n",
    "若$y_1 \\ne y_2$：\n",
    "\n",
    "$$L=max(0,\\alpha_2^{old}-\\alpha_1^{old})$$\n",
    "$$H=min(C,C+\\alpha_2^{old}-\\alpha_1^{old})$$\n",
    "\n",
    "若$y_1 = y_2$：\n",
    "\n",
    "$$L=max(0,\\alpha_2^{old}+\\alpha_1^{old}-C)$$\n",
    "$$H=min(C,\\alpha_2^{old}+\\alpha_1^{old})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 接着，我们求出沿着约束方向但不受盒子约束时(在直线上，而不一定必须在线段上)$\\alpha_2$的最优解为$\\alpha_2^{new,unc}$。记(书上)：\n",
    "\n",
    "$$g(x) = \\sum_{i=1}^N\\alpha_iy_iK(x_i,x)+b$$\n",
    "\n",
    "(但我认为，最好写成)：\n",
    "\n",
    "$$g^{old}(x) = \\sum_{i=1}^N\\alpha_i^{old}y_iK(x_i,x)+b^{old}$$\n",
    "\n",
    "令(书上)：\n",
    "\n",
    "$$E_i = g(x_i)-y_i,\\quad i=1,2$$\n",
    "\n",
    "(同样)：\n",
    "\n",
    "$$E_i^{old} = g^{old}(x_i)-y_i,\\quad i=1,2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，为了更加方便地表示子规划问题的目标函数，我们再引入(书上的写法会引入混淆)：\n",
    "\n",
    "$$v_i^{old} = \\sum_{j=3}^Ny_j\\alpha_iK_{ij}=g^{old}(x_i)-\\sum_{j=1}^2\\alpha_j^{old}y_iK_{ij}-b^{old},\\quad i=1,2$$\n",
    "\n",
    "易知，$v_i^{old}$与$\\alpha_1,\\alpha_2$无关，因为$\\alpha_1^{old},\\alpha_2^{old}$是固定的。道这样，子规划问题的目标函数可以转化为：\n",
    "\n",
    "$$W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1\\alpha_1v_1^{old}+y_2\\alpha_2v_2^{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为$\\alpha_1y_1+\\alpha_2y_2=\\zeta$且$y_i^2=1$，所以我们可以将$\\alpha_1$用$\\alpha_2$表示：\n",
    "\n",
    "$$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$$\n",
    "\n",
    "将上式代入$W(\\alpha_1,\\alpha_2)$，消去$\\alpha_1$。然后将$W(\\alpha_2)$对$\\alpha_2$求偏导置零(注意代入$v_i^{old}$，并使用$\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\zeta$)，解得：\n",
    "\n",
    "$$\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1^{old}-E_2^{old})}{K_{11}+K_{22}-2K_{12}}$$\n",
    "\n",
    "再简化一下：\n",
    "\n",
    "$$\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1^{old}-E_2^{old})}{\\eta}$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\\eta = K_{11}+K_{22}-2K_{12} = ||\\Phi(x_1)-\\Phi(x_2)||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 最后，综合最开始的$L \\le \\alpha_2^{new} \\le H$，得到：\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha_2^{new}=\n",
    "    \\begin{cases}\n",
    "    H,                  & \\alpha_2^{new,unc}>H,\\\\\n",
    "    \\alpha_2^{new,unc}, & L \\le \\alpha_2^{new,unc} \\le H\\\\\n",
    "    L,                  & \\alpha_2^{new,unc}<L,\\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "由$\\alpha_1^{old}y_1+\\alpha_2^{old}y_2 = \\alpha_1^{new}y_1+\\alpha_2^{new}y_2$得：\n",
    "\n",
    "$$\\alpha_1^{new} = \\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 结束，得到$(\\alpha_1^{new},\\alpha_2^{new})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 变量的选择方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个变量的选择：第一个，严重违反KKT；第二个，根据第一个选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 第一个变量的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO的术语：__外层循环__——在训练样本中选取违反KKT条件最严重的样本点。具体的，检验样本点$(x_i,y_i)$：\n",
    "\n",
    "* 若$\\alpha_i=0$，检查$g(x_i)\\ge1$\n",
    "* 若$0<\\alpha_i<C$，检查$g(x_i)=1$\n",
    "* 若$\\alpha_i=C$，检查$g(x_i)\\le1$\n",
    "\n",
    "另外，先遍历所有满足$0<\\alpha_i<C$的样本点，即在margin上的；若这些free SV都满足，则遍历整个训练集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 第二个变量的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO的术语：__内层循环__——已经找到了第一个变量$\\alpha_1$，内层循环是找$\\alpha_2$；选择标准是，希望$\\alpha_2$有足够大的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上一部分的推导可知，$\\alpha^{new}_2$依赖于$|E_1^{old}-E_2^{old}|$。一种简单的选择方法是，选择这样的$\\alpha_2$，使其对应的$|E_1^{old}-E_2^{old}|$最大(因为$\\alpha_1$已经确定，也就是那个样本确定了，所以$E_1^{old}$也确定了)：\n",
    "\n",
    "* 如果$E_1^{old}$是正的，那么选择最小的$E_i^{old}$作为$E_2^{old}$；\n",
    "* 如果$E_1^{old}$是负的，那么选择最大的$E_i^{old}$作为$E_2^{old}$；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊情况下，如果内层循环选到的$\\alpha_2$不能使目标函数有足够的下降，那么换一种方式继续选择其他的$\\alpha_2$：\n",
    "\n",
    "* 遍历margin上的free SV，依次将其作为$\\alpha_2$，直到拿到一个使得目标函数有足够的下降；\n",
    "* 若找不到合适的，则遍历整个数据集；\n",
    "* 如果还没有，则放弃第一个变量，重选$\\alpha_1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 选完两个变量之后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①重新计算b；②更新$E_i$列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①重新计算b："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(书上的假设：更新完$\\alpha_1$与$\\alpha_2$后，他们对应的样本点不再违反KKT条件，所以可以用KKT条件算新的$b$。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$0<\\alpha_1^{new}<C$时，由$y_1g(x_1)=1$知：\n",
    "\n",
    "$$y_1 = \\sum_{i=1}^{N}\\alpha^{new/old}_iy_iK_{i1}+b^{new}_1=\\alpha_1^{new}y_1K_{11}+\\alpha_2^{new}y_2K_{12}+\\sum_{i=3}^{N}\\alpha^{old}_iy_iK_{i1}+b^{new}_1$$\n",
    "\n",
    "又：\n",
    "\n",
    "$$E_1^{old} = g^{old}(x_1)-y_1$$\n",
    "\n",
    "联立得：\n",
    "\n",
    "$$b_1^{new} = -E_1^{old}-y_1K_{11}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{21}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，若$0<\\alpha_2^{new}<C$：\n",
    "\n",
    "$$b_2^{new} = -E_2^{old}-y_2K_{12}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{22}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若$0<\\alpha_1^{new}<C$与$0<\\alpha_2^{new}<C$同时成立，那么$b_1^{new}=b_2^{new}$。若其中一个是0或C，则$b_1^{new} \\ne b_2^{new}$，两者之间的所有b值都是符合KKT的，我们选择他们的__中点__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "②更新$E_i$列表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_i^{new} = g^{new}(x_i)-y_i = \\sum_{i=1}^N\\alpha_i^{new}y_iK(x_i,x)+b^{new}-y_i,\\quad i=1,2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然：\n",
    "\n",
    "$$\\sum_{i=1}^N = \\sum_{SV}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
