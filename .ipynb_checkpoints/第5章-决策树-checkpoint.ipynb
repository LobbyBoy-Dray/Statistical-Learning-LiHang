{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decision Tree\n",
    "> * 决策树是一种基本的classification和regression方法；\n",
    "> * 既可以看做是if-then规则的集合，也可以看做基于特征空间划分的条件概率分布；\n",
    "> * 可读性高，易理解；\n",
    "> * DT的生成 & DT的修剪；\n",
    "> * ID3, C4.5, CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 决策树模型与学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. 决策树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树由两类结点构成：\n",
    "\n",
    "1. __内部结点(internal node)__: \n",
    "    * 对应一个属性测试；\n",
    "    * 或指向叶结点(导出最终结论，即类别)，或指向其他内部结点(导出进一步的判定问题，注意其考虑范围是在上次决策结果的限定范围之内)；\n",
    "    * 其中，内部结点中有一个特殊的结点，即根结点(root node)；\n",
    "2. __叶结点(leaf node)__：\n",
    "    * 对应决策结果。\n",
    "    \n",
    "注：每个结点均包含一定的样本集合，内部结点包含的样本集合根据属性测试的结果被划分到子结点中，根结点包含样本全集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. 决策树与if-then规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树可以看做if-then规则的集合。该集合中的每一个元素，是从根结点到每个叶结点的路径(所对应的一个判定测试序列)。该集合的一个重要性质是__互斥且完备__，即每一个实例都被一条路径覆盖，且有且只有一条路径与之对应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Path View__: 有$T$条路径\n",
    "\n",
    "$$G(\\mathbf{x})=\\sum^T_{t=1}I(\\mathbf{x}\\ on\\ path\\ t)·leaf_t(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三. 决策树与条件概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一棵决策树对应着__特征空间的一种划分方式__，特征空间被划分为一个个互不相交的__单元__——单元就对应着“路径集合”中的路径。每个单元中都有一个关于类别的概率分布，该单元的预测输出为最大概率分布对应的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四. 决策树的递归视角"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recursive View__: 当前有$C$条分支(branch)\n",
    "\n",
    "$$G(\\mathbf{x})=\\sum^C_{c=1}I\\Big(b(\\mathbf{x})=c\\Big)·G_c(\\mathbf{x})$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $b(\\mathbf{x})$：分支决策条件，branching criteria.\n",
    "* $G_c(\\mathbf{x})$：子树，sub-tree hypothesis at the c-th branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五. 决策树学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树学习本质上是从训练数据集中归纳出一组分类规则。需要注意，与训练集不相矛盾的决策树可能有多个，也可能一个都没有。我们需要的，是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树学习的算法通常是一个“递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类”的过程。递归的终止条件一般是①所有训练数据子集被“基本”正确分类，②没有合适的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的学习算法主要有三种：ID3，C4.5，CART。CART与前两个有稍大不同。例如，在最优特征选择方面，CART允许重复利用特征，而ID3与C4.5则不允许特征的重复利用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__生成决策树的一般算法(ID3与C4.5, 不适用于CART, 因为CART不需要属性集, 它可以重复利用特征)__\n",
    "\n",
    "————————————————————————————————————\n",
    "\n",
    "__输入__: ①训练集$D=\\{(\\mathbf{x_1},y_1),(\\mathbf{x_2},y_2),\\cdots,(\\mathbf{x_m},y_m)\\}$; ②属性集$A=\\{a_1,a_2,\\cdots,a_d\\}$.\n",
    "\n",
    "__过程__: 函数$TreeGenerate(D,A)$\n",
    "\n",
    "&ensp;&ensp;1: 生成结点node;\n",
    "\n",
    "&ensp;&ensp;2: __if__ $D$中样本全属于同一类别$C$ __then__\n",
    "\n",
    "&ensp;&ensp;3: &ensp;&ensp;&ensp;将node标记为$C$类叶结点; __return__\n",
    "\n",
    "&ensp;&ensp;4: __end if__\n",
    "\n",
    "&ensp;&ensp;5: __if__ $A=\\emptyset$ __OR__ $D$中样本在$A$上取值相同 __then__\n",
    "\n",
    "&ensp;&ensp;6: &ensp;&ensp;&ensp;将node标记为叶结点，其类别标记为$D$中样本数最多的类; __return__\n",
    "\n",
    "&ensp;&ensp;7: __end if__\n",
    "\n",
    "&ensp;&ensp;8: 从$A$中选择最优划分属性$a_*$;\n",
    "\n",
    "&ensp;&ensp;9: __for__ $a_*$的每一个值$a_*^v$ __do__\n",
    "\n",
    "10: &ensp;&ensp;&ensp;为node生成一个分支; 令$D_v$表示$D$中在$a_*$上取值为$a_*^v$的样本子集;\n",
    "\n",
    "11: &ensp;&ensp;&ensp;__if__ $D_v$为空 __then__\n",
    "\n",
    "12: &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将分支结点标记为叶结点，其类别标记为$D$中样本数最多的类; __return__\n",
    "\n",
    "13: &ensp;&ensp;&ensp;__else__\n",
    "\n",
    "14: &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;以$TreeGenerate(D_v,A\\backslash \\{a_*\\})$为分支结点\n",
    "\n",
    "15: &ensp;&ensp;&ensp;__end if__\n",
    "\n",
    "16: __end for__\n",
    "\n",
    "__输出__: 以node为根结点的一棵决策树\n",
    "\n",
    "————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__关于剪枝__：为了避免过拟合，需要对已生成的树进行自下而上的剪枝，使树变得简单——去掉过于细分的叶结点，使其回退到父结点甚至更高结点，然后将其改为叶结点。决策树的生产只考虑局部最优，决策树的剪枝考虑全局最优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 最优划分特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论是哪一种算法，都要先面对一个问题：怎样选择最优划分的特征？ID3，C4.5，CART各有不同，但ID3和C4.5十分类似，都考虑了__熵__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__熵(entropy)__：度量一个随机变量的不确定性。假设$X$是一个取值有限个值的离散随机变量，其概率分布为：\n",
    "\n",
    "$$P(X=x_i)=p_i,\\quad i=1,2,\\cdots,n$$\n",
    "\n",
    "则随机变量$X$的熵为：\n",
    "\n",
    "$$H(X) = -\\sum^n_{i=1}p_i\\ log\\ p_i$$\n",
    "\n",
    "由上式可知，$X$的熵只依赖于$X$的分布，与$X$的取值无关；熵越大，随机变量的不确定性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__条件熵(conditional entropy)__：设有随机向量$(X,Y)$，其联合概率分布为:\n",
    "\n",
    "$$P(X=x_i,Y=y_j)=p_{ij},\\quad i=1,2,\\cdots,n;\\quad j=1,2\\cdots,m$$\n",
    "\n",
    "条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，即$X$给定条件下$Y$的条件熵为：\n",
    "\n",
    "$$H(Y|X) = \\sum_{i=1}^nP(X=x_i)H(Y|X=x_i)$$\n",
    "\n",
    "(感觉这里写成$H(X|Y)$好一些)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当熵和条件熵中的概率由(训练)数据估计(极大似然估计)得到时，熵被称为经验熵，条件熵被称为经验条件熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__信息增益__：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__算法__：计算信息增益\n",
    "\n",
    "__输入__: ①训练集$D$，②(某一)特征$A$；\n",
    "\n",
    "__输出__: (某一)特征$A$对训练集$D$的信息增益$g(D,A)$\n",
    "\n",
    "__记号__：\n",
    "\n",
    "1. $|D|$，数据集容量；\n",
    "2. $K$，数据集有$K$个类，每个类称为$C_k$；\n",
    "3. 每个类有$|C_k|$个数据，因此有$\\sum_{k=1}^K|C_k|=|D|$；\n",
    "4. 特征$A$有$n$个取值，因此根据特征$A$的取值可以将$D$分拆为$n$个子集：$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，因此有$\\sum_{i=1}^n|D_i|=|D|$；\n",
    "5. $D_{ik}$表示$D_i$中属于类$C_k$的样本集合，因此有$\\sum_{i=1}^n\\sum_{k=1}^K|D_{ik}|=|D|$；\n",
    "\n",
    "__过程__: \n",
    "\n",
    "1. 计算数据集$D$的经验熵$H(D)$\n",
    "\n",
    "$$H(D)=-\\sum_{k=1}^K \\frac{|C_k|}{|D|} log_2 \\frac{|C_k|}{|D|}$$\n",
    "\n",
    "2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$\n",
    "\n",
    "$$H(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n \\frac{|D_i|}{|D|}  \\sum_{k=1}^K \\frac{|C_{ik}|}{|D_i|} log_2 \\frac{|C_{ik}|}{|D_i|}$$\n",
    "\n",
    "3. 计算信息增益\n",
    "\n",
    "$$g(D,A) = H(D)-H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. 信息增益比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__信息增益比__：特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$，定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：\n",
    "\n",
    "$$g_R(D,A) = \\frac{g(D,A)}{H_A(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 决策树的训练-ID3与C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心思想：在每个结点上使用__信息增益__选择最佳特征，利用其所有可能取值构建分支，并对子结点递归；直到(或)：①所有特征的信息增益均很小(结点内数据属于同一类)，②没有特征可以选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特点：\n",
    "* 信息增益作为最优特征选择依据；\n",
    "* 贪心算法——每次选取的特征都是当前的最优选择，并不关心是否达到全局最优；\n",
    "* 不能处理feature为连续值的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择最佳特征的方法：__信息增益__→__信息增益比__。\n",
    "\n",
    "特点：\n",
    "* ID3的缺点，使用信息增益会优先选择有较多属性值的feature，因此C4.5改用信息增益比；\n",
    "* 能够处理feature是连续值的情况——(From: Scikit-learn) C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals——最简单的策略：__二分法(也就是decision stump)__，也是C4.5采取的方法(需要注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性——也就是不会从$A$中删掉这个特征)：\n",
    "\n",
    "(以下图片from周志华《机器学习》)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![连续值的处理](./pic/ch5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![连续值的处理](./pic/ch5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully-grown tree容易过拟合，因此需要简化树，对树简化的过程被称为__剪枝(pruning)__——具体的，剪枝就是从已经生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种简单的剪枝算法——极小化cost function。假设树$T$的叶结点个数为$|T|$，$t$是树$T$的第$t$个叶结点，该叶结点上有$N_t$个样本点，其中又有第$k$类的样本$N_{tk}$个($k=1,2,\\cdots,K$)。叶结点$t$上的经验熵被记做$H_t(T)$；$\\alpha \\ge 0$，则决策树$T$的cost function可以写作：\n",
    "\n",
    "$$C_{\\alpha}(T) = \\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha|T| = -\\sum_{t=1}^{|T|}\\sum_{k=1}^K N_{tk} log_2 \\frac{N_{tk}}{N_t}+\\alpha|T| = C(T)+\\alpha|T|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向上回缩，直到不能继续为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART = classification and regression tree，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART的不同点：\n",
    "* 二叉树，每个内部结点只引出两条分支，左分支是取值为“是”的分支，右分支是取值为“否”的分支——__decision stump__(numerical features)/__decision subset__(categorical features)；\n",
    "* CART允许重复利用特征——does not compute rule sets $A$；\n",
    "* 输出变量是连续的(回归树)：it supports numerical target variables (regression)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__包山包海!__Scikit-learn也使用CART来生成决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆，决策树的递归写法：\n",
    "\n",
    "$$G(\\mathbf{x}) = \\sum_{c=1}^C I\\Big(b(\\mathbf{x})=c\\Big)·G_c(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 分类树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART算法下，每个$C$都等于2。那么怎么将结点的数据集一刀两段呢？用decision stump来切。如何用决策树桩来切？需要找到最优特征与最优切割点，这两个是一起找到的——使用__基尼系数(Gini Index)__，一种不纯度的度量方式(Impurity Function)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gini Index__，样本集合$D$的基尼系数为：\n",
    "\n",
    "$$Gini(D) = 1-\\sum_{k=1}^K \\Big( \\frac{|C_k|}{|D|} \\Big)^2$$\n",
    "\n",
    "若$D$根据特征$A$的某一可能取值$a$被分割成$D_1$与$D_2$，则在$A,a$的条件下，集合$D$的Gini指数为：\n",
    "\n",
    "$$G(D,A=a) = \\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$\n",
    "\n",
    "上述值越大，说明$A=a$这个切点越不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法略，需要注意的点：\n",
    "* 每个内部结点上，要在所有可能的特征$A$以及它们所有可能的切分点$a$中，一步寻找到最优特征和最优切分点；\n",
    "* 终止条件：\n",
    "    * 结点的Gini系数为0(100%纯，或设定一个阈值)；\n",
    "    * 所有的特征都一样(没下刀处)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Impurity(D) = \\frac{1}{|D|}\\sum_{n=1}^N (y_n-\\bar{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART剪枝将从fully-grown tree的下部剪去一些“子树”(注意，这里不是叶结点，而是子树)，使决策树变小。包括两个步骤：\n",
    "* 从生成算法产生的fully-grown tree $T_0$下部开始，不断剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,\\cdots,T_n\\}$；\n",
    "* 通过交叉验证，对上述子树序列进行测试，选出最优子树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: 不断剪枝形成子树序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个子树$T$，它的cost function为：\n",
    "\n",
    "$$C_\\alpha(T) = C(T)+\\alpha |T|$$\n",
    "\n",
    "其中，$C(T)$为子树的预测误差，相当于$E_{in}$；$|T|$是子树的叶结点数量；$\\alpha \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们从整体树$T_0$开始剪枝。对$T_0$任意内部结点$t$，以$t$为单结点树的cost function是：(剪掉以该结点为根结点的子树后，树的cost function=$C_\\alpha(t)+others$)\n",
    "\n",
    "$$C_\\alpha(t)=C(t)+\\alpha$$\n",
    "\n",
    "以$t$为根结点的子树$T_t$的cost function为：(剪掉以该结点为根结点的子树前，即原来的树的cost function=$C_\\alpha(T_t)+others$)\n",
    "\n",
    "$$C_\\alpha(T_t)=C(T_t)+\\alpha|T_t|$$\n",
    "\n",
    "因此：\n",
    "* 若$C_\\alpha(T_t)<C_\\alpha(t)$，则不剪枝；\n",
    "* 若$C_\\alpha(T_t)=C_\\alpha(t)$，临界点；\n",
    "* 若$C_\\alpha(T_t)>C_\\alpha(t)$，则剪枝。\n",
    "\n",
    "计算临界点的$\\alpha$:\n",
    "\n",
    "$$\\alpha = \\frac{C(t)-C(T_t)}{|T_t|-1}$$\n",
    "\n",
    "因此，当$\\alpha$在$[0,该临界点]$时，不剪枝的$T_0$为最优子树；当$\\alpha$在$[该临界点,某个值(其实是下一临界点)]$时，剪枝得到的$T_1$是最优子树。\n",
    "\n",
    "__这里我们将遍历所有的内部结点，找到某个结点，它所对应的临界点的$\\alpha$最小__，然后剪掉它，得到$\\alpha_1$与$T_1$。\n",
    "\n",
    "同理，我们将得到$\\alpha_2$与$T_2$……\n",
    "\n",
    "|\\_\\_\\_\\_\\_\\_|\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "$\\alpha_0$-----$\\alpha_1$-----\n",
    "\n",
    "---$T_0$------$T_1$\n",
    "\n",
    "↓\n",
    "\n",
    "|\\_\\_\\_\\_\\_\\_|\\_\\_\\_\\_\\_\\_|\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "$\\alpha_0$-----$\\alpha_1$-----$\\alpha_2$-----\n",
    "\n",
    "---$T_0$------$T_1$------$T_2$\n",
    "\n",
    "当$T_k$是由根结点及其两个叶结点构成的树时，停止，令$T_n=T_k$，得到子树序列$\\{T_0,T_1,\\cdots,T_n\\}$。\n",
    "\n",
    "__Q: 如何保证$\\alpha_{i+1}>\\alpha_{i}$???__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: 在剪枝得到的子树序列中通过交叉验证选择最优子树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前仅实现未剪枝的ID3树生成算法，对应习题1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据准备__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 青年=1,中年=2,老年=3\n",
    "# 无工作=0,有工作=1\n",
    "# 无自己的房子=0,有自己的房子=1\n",
    "# 信贷一般=1,信贷好=2,信贷非常好=3\n",
    "# 类别否=0,类别是=1\n",
    "data = np.array([[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n",
    "                 [0,0,1,1,0,0,0,1,0,0,0,0,1,1,0],\n",
    "                 [0,0,0,1,0,0,0,1,1,1,1,1,0,0,0],\n",
    "                 [1,2,2,1,1,1,2,2,3,3,3,2,2,3,1],\n",
    "                 [0,0,1,1,0,0,0,1,1,1,1,1,1,1,0]]).T\n",
    "X = data[:,:4]\n",
    "y = data[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ID3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"树结点\n",
    "    Parameters\n",
    "    --------------\n",
    "    entropy : float\n",
    "        该结点上数据的经验熵\n",
    "    samples : int\n",
    "        该结点上数据的数量\n",
    "    value : list\n",
    "        该结点上数据的分布情况\n",
    "    best_feature : int\n",
    "        该结点的最佳切分维度(的index)\n",
    "    children_nodes : list\n",
    "        子结点列表\n",
    "    \"\"\"\n",
    "    def __init__(self, entropy, samples, value, best_feature, children_nodes):\n",
    "        self.entropy  = entropy\n",
    "        self.samples  = samples\n",
    "        self.value    = value\n",
    "        self.best_feature = best_feature\n",
    "        self.children_nodes = children_nodes\n",
    "\n",
    "def class_count(y,class_list):\n",
    "    \"\"\"计算每个输出类别所包含的样本数量\n",
    "    Parameters\n",
    "    --------------\n",
    "    y : list or ndarray\n",
    "        输出集\n",
    "    class_list : list\n",
    "        输出类别清单\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    result : list\n",
    "        每个输出类别所包含的样本数量, 顺序对应于class_list\n",
    "    \"\"\"\n",
    "    result = [0]*len(class_list)\n",
    "    fre = pd.value_counts(y)\n",
    "    for i,j in fre.items():\n",
    "        result[class_list.index(i)] = j\n",
    "    return result\n",
    "        \n",
    "def entropy(y):\n",
    "    \"\"\"计算经验熵\n",
    "    Parameters\n",
    "    --------------\n",
    "    y : list or ndarray\n",
    "        输出集\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    entr : float\n",
    "        经验熵\n",
    "    \"\"\"\n",
    "    value_fre = pd.value_counts(y,normalize=True)\n",
    "    entr = np.sum(-np.log2(value_fre)*value_fre)\n",
    "    return entr\n",
    "\n",
    "def get_best_feature(X,y,A):\n",
    "    \"\"\"获取最优切分特征\n",
    "    Parameters\n",
    "    --------------\n",
    "    X : ndarray\n",
    "        输出数据\n",
    "    y : list or ndarray\n",
    "        输出集\n",
    "    A : list(不能是ndarray)\n",
    "        特征集\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    best_feature : int\n",
    "        最优切分特征(索引)\n",
    "    \"\"\"\n",
    "    H = entropy(y)\n",
    "    best_feature = -1\n",
    "    best_info_gain = -1\n",
    "    for feature in A:\n",
    "        con_entr = 0\n",
    "        value_fre = pd.value_counts(X[:,feature],normalize=True)\n",
    "        for i,j in value_fre.items():\n",
    "            y_temp = y[X[:,feature]==i]\n",
    "            con_entr += j*entropy(y_temp)\n",
    "        info_gain = H-con_entr\n",
    "        if info_gain > best_info_gain:\n",
    "            best_feature = feature\n",
    "            best_info_gain = info_gain\n",
    "    return best_feature\n",
    "\n",
    "def split_data(X,y,best_feature):\n",
    "    \"\"\"根据最优切分特征切分数据集\n",
    "    Parameters\n",
    "    --------------\n",
    "    X : ndarray\n",
    "        输出数据\n",
    "    y : list or ndarray\n",
    "        输出集\n",
    "    best_feature : int\n",
    "        最优切分特征(索引)\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    X_set : list\n",
    "        最优切分特征(索引)\n",
    "    y_set : list\n",
    "    \"\"\"\n",
    "    X_set = []\n",
    "    y_set = []\n",
    "    unique_value = np.unique(X[:,best_feature])\n",
    "    for i in unique_value:\n",
    "        index = X[:,best_feature] == i\n",
    "        X_set.append(X[index])\n",
    "        y_set.append(y[index])\n",
    "    return X_set,y_set\n",
    "    \n",
    "def ID3DecisonTree(X,y,A,class_list):\n",
    "    \"\"\"根据ID3算法构造决策树\n",
    "    Parameters\n",
    "    --------------\n",
    "    X : ndarray\n",
    "        输出集\n",
    "    y : list or ndarray\n",
    "        输出集\n",
    "    A : list(不能是ndarray)\n",
    "        特征集\n",
    "    class_list : list\n",
    "        输出类别清单\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    Node : Node\n",
    "        结点\n",
    "    \"\"\"\n",
    "    entr = entropy(y)\n",
    "    value = class_count(y,class_list)\n",
    "    # 递归终止条件\n",
    "    if entr == 0 or A == [] or len(np.unique(X[:,A],axis=0))==1:\n",
    "        return Node(entropy=entr, samples=len(y), value=value, best_feature=None, children_nodes=[])\n",
    "    # 获取最优切分特征\n",
    "    best_feature = get_best_feature(X,y,A)\n",
    "    # 将其从特征集中去除\n",
    "    A.remove(best_feature)\n",
    "    # 获取子结点\n",
    "    children_nodes = []\n",
    "    X_set,y_set = split_data(X,y,best_feature)\n",
    "    for X_sub, y_sub in zip(X_set,y_set):\n",
    "        children_nodes.append(ID3DecisonTree(X_sub,y_sub,A,class_list))\n",
    "    return Node(entropy=entr, samples=len(y), value=value, best_feature=best_feature, children_nodes=children_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 特征集\n",
    "    A = list(range(len(X[0])))\n",
    "    # 输出类别清单\n",
    "    class_list = np.unique(y).tolist()\n",
    "    # 构造决策树\n",
    "    DT = ID3DecisonTree(X,y,A,class_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
